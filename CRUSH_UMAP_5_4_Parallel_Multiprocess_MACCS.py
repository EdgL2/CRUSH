# -*- coding: utf-8 -*-
"""
Parallel streaming molecular fragmentation using the RECAP, BRICS and CRUSH SMIRKS
you provided. Computes ECFP4 (Morgan radius=2) 2048-bit fingerprints, packs them
as bytes and appends to a binary file. Metadata is streamed to CSV in batches.

Design goals:
 - Use the exact SMIRKS rules you gave.
 - Precompile Reaction objects once.
 - Use multiprocessing for fragmentation + fingerprint calc.
 - Single writer (main process) writes .bin and .csv to keep order/alignment.
 - Chunked CSV reading to avoid high memory use.

Reads packed binary fingerprints (.bin) generated by this script
and computes low-dimensional embeddings using Incremental PCA + UMAP.

"""

import os
import math
import random
import time
import traceback
import multiprocessing as mp

import pandas as pd
import numpy as np
from rdkit import Chem, RDLogger, DataStructs
from rdkit.Chem import AllChem

from sklearn.decomposition import IncrementalPCA
import umap

# Suppress RDKit warnings
lg = RDLogger.logger()
lg.setLevel(RDLogger.CRITICAL)


# --------------------------
# CONFIGURATION
# --------------------------

FP_NBITS = 166              # number of fingerprint bits (as in original code)
FP_NBYTES = FP_NBITS // 8    # 21 bytes per fingerprint
BATCH_WRITE = 1000           # how many metadata rows to buffer before writing CSV
CHUNKSIZE = 1000             # pandas read_csv chunksize
N_WORKERS = max(mp.cpu_count() - 1, 1)
MODES = ("RECAP", "BRICS", "CRUSH")

OUT_CSV = "fragments.csv"
OUT_BIN = "fps.bin"


BATCH_SIZE = 5000           # Number of fingerprints per batch to fit/transform
N_COMPONENTS_PCA = 50      # Target dimensions after PCA
N_COMPONENTS_UMAP = 2       # UMAP target dims (2D embedding)
RANDOM_SEED = 42

FPS_BIN = "fps.bin"         # binary file from fragmentation
META_CSV = "fragments.csv"  # metadata CSV with fragment SMILES
OUT_PCA = "pca_fragments.npy"
OUT_UMAP = "umap_fragments.npy"
OUT_CSV_EMBED = "fragments_with_umap.csv"

# ------------------------------
# Auxiliary functions
# ------------------------------

def frag_rules(mode="ALL", random_seed=None):
    """
    Return the SMIRKS rule list for the requested mode.
    This returns lists of tuples (name, smirks) using the exact rules you gave.
    """

    recap_smirks = [
    	('Urea', '[#7;+0;D2,D3:1]!@C(!@=O)!@[#7;+0;D2,D3:2]>>[1*][#7:1].[2*][#7:2]'),
        ('Amide', '[C;!$(C([#7])[#7]):1](=!@[O:2])!@[#7;+0;!D1:3]>>[3*][C:1]=[O:2].[4*][#7:3]'),
        ('Ester', '[C:1](=!@[O:2])!@[O;+0:3]>>[5*][C:1]=[O:2].[6*][O:3]'),
        ('Amines', '[N;!D1;+0;!$(N-C=[#7,#8,#15,#16])](-!@[*:1])-!@[*:2]>>[7*][*:1].[8*][*:2]'),
    	('Cyclic amines', '[#7;R;D3;+0:1]-!@[*:2]>>[9*][#7:1].[10*][*:2]'),
    	('Ether', '[#6:1]-!@[O;+0]-!@[#6:2]>>[11*][#6:1].[12*][#6:2]'),
    	('Olefin', '[C:1]=!@[C:2]>>[13*][C:1].[14*][C:2]'),
    	('Aromatic nitrogen - aliphatic carbon', '[n;+0:1]-!@[C:2]>>[15*][n:1].[16*][C:2]'),
    	('Lactam nitrogen - aliphatic carbon', '[O:3]=[C:4]-@[N;+0:1]-!@[C:2]>>[17*][O:3]=[C:4]-[N:1].[18*][C:2]'),
    	('Aromatic carbon - aromatic carbon', '[c:1]-!@[c:2]>>[19*][c:1].[20*][c:2]'),
    	('Aromatic nitrogen - aromatic carbon', '[n;+0:1]-!@[c:2]>>[21*][n:1].[22*][c:2]'),
    	('Sulphonamide', '[#7;+0;D2,D3:1]-!@[S:2](=[O:3])=[O:4]>>[23*][#7:1].[24*][S:2](=[O:3])=[O:4]')
    ]


    brics_smirks = [
    	('L1', '[C;D3:1](=O)-;!@[#0,#6,#7,#8:2]>>[25*][*:1].[26*][*:2]'),
    	('L2a', '[N;D3;R;$(N(@[C;!$(C=*)])@[C;!$(C=*)]):1]-[*:2]>>[27*][*:1].[28*][*:2]'),
    	('L3', '[O;D2:1]-;!@[#0,#6,#1:2]>>[29*][*:1].[30*][*:2]'),
    	('L4', '[C;!D1;!$(C=*):1]-;!@[#6:2]>>[31*][*:1].[32*][*:2]'),
    	('L5', '[N;!D1;!$(N=*);!$(N-[!#6;!#16;!#0;!#1]);!$([N;R]@[C;R]=O):1]-[*:2]>>[33*][*:1].[34*][*:2]'),
    	('L6', '[C;D3;!R:1](=O)-;!@[#0,#6,#7,#8:2]>>[35*][*:1].[36*][*:2]'),
    	('L7a', '[C;D2,D3:1]-[#6:2]>>[37*][*:1].[38*][*:2]'),
    	('L7b', '[C;D2,D3:1]-[#6:2]>>[39*][*:1].[40*][*:2]'),
    	('L8', '[C;!R;!D1;!$(C!-*):1]-[*:2]>>[41*][*:1].[42*][*:2]'),
    	('L9', '[n;+0;$(n(:[c,n,o,s]):[c,n,o,s]):1]-[*:2]>>[43*][*:1].[44*][*:2]'),
    	('L10', '[N;R;$(N(@C(=O))@[C,N,O,S]):1]-[*:2]>>[45*][*:1].[46*][*:2]'),
    	('L11', '[S;D2:1](-;!@[#0,#6])-[*:2]>>[47*][*:1].[48*][*:2]'),
    	('L12', '[S;D4:1]([#6,#0])(=O)(=O)-[*:2]>>[49*][*:1].[50*][*:2]'),
    	('L13', '[C;$(C(-;@[C,N,O,S])-;@[N,O,S]):1]-[*:2]>>[51*][*:1].[52*][*:2]'),
    	('L14', '[c;$(c(:[c,n,o,s]):[n,o,s]):1]-[*:2]>>[53*][*:1].[54*][*:2]'),
    	('L14b', '[c;$(c(:[c,n,o,s]):[n,o,s]):1]-[*:2]>>[55*][*:1].[56*][*:2]'),
    	('L15', '[C;$(C(-;@C)-;@C):1]-[*:2]>>[57*][*:1].[58*][*:2]'),
    	('L16', '[c;$(c(:c):c):1]-[*:2]>>[59*][*:1].[60*][*:2]'),
    	('L16b', '[c;$(c(:c):c):1]-[*:2]>>[61*][*:1].[62*][*:2]')
   ]


    CRUSH_smirks = [
    	('Amide', '[#6:1]!@[C!R:2](=[O:3])!@[N:4]>>[*:1][*:2](=[*:3])[63*].[64*][*:4]'),
    	('Ester', '[#6:1]!@[C!R:2](=[O:3])!@[O!R:4][#6:5]>>[*:1][*:2](=[*:3])[65*].[66*][*:4][*:5]'),
    	('Amine', '[NX3,N+X4:1]([C,H,!*3;!$(*=[O,N]):2])([C,H,!*3;!$(*=[O,N]):3])!@[C;!$(C=[O,N]):4]>>[*:1]([*:2])([*:3])[67*].[68*][*:4]'),
    	('Amine_n_ring', '[N;D3;R;$(N(@[C;!$(C=*)])@[C;!$(C=*)]):1]-[*:2]>>[69*][*:1].[70*][*:2]'),
    	('Urea', '[N:1]!@[C!R:2](=[O:3])!@[N:4]>>[*:1][*:2](=[*:3])[71*].[72*][*:4]'),
    	('Ether', '[#6,#1:1][#6:2]!@[O!R:3]!@[#6:4][#6,#1:5]>>[*:1][*:2][*:3][73*].[74*][*:4][*:5]'),
    	('Hetero-substituted alkyl C–X cleavage', '[C;$(C(-;@[C,N,O,S])-;@[N,O,S]):1]-[*:2]>>[75*][*:1].[76*][*:2]'),
    	('Olefin', '[C:1]!@=[C:2]>>[*:1][77*].[78*][*:2]'),
    	('Quaternary nitrogen', '[N+X4&H0:1]!@[C:2]>>[*:1][79*].[80*][*:2]'),
    	('Aromatic nitrogen - aliphatic carbon', '[n:1]!@[C:2]>>[*:1][81*].[82*][*:2]'),
    	('Lactam nitrogen - aliphatic carbon', '[NR:1](@[CR;!$(C=[O,N]):2])(!@[C:3])@[CR:4]=[O:5]>>[83*][*:1]([*:2])[*:4]=[*:5].[84*][*:3]'),
    	('Aromatic carbon - aromatic carbon', '[c:1]!@[c:2]>>[85*][*:1].[86*][*:2]'),
    	('Sulphonamide', '[S!R:1](=[O:2])(=[O:3])!@[N:4]!@[#6;!$(C=[O,N]):5]>>[*:1](=[*:2])(=[*:3])[87*].[88*][*:4][*:5]'),
    	('Aromatic carbon - aliphatic carbon', '[c:1]!@[CX4;!$(C~[!#1&!#6]):2]>>[*:1][89*].[90*][*:2]'),
    	('Aromatic carbon - aliphatic amine', '[c:1]!@[N:2]>>[*:1][91*].[92*][*:2]'),
    	('Alkyne', '[C:1]!@#[C:2]>>[*:1][93*].[94*][*:2]'),
    	('Thioether', '[#6;!R:1]!@[S:2]!@[#6;!R:3]>>[95*][*:1].[96*][*:2].[97*][*:3]'),
    	('Carbamate', '[O:1]!@[C!R:2](=[O:3])!@[N:4]>>[*:1][*:2](=[*:3])[98*].[99*][*:4]'),
    	('Carbamate_non_ring', '[O;!R:1]!@[C;!R:2](=[O:3])!@[N;!R:4]>>[100*][*:1].[101*][*:2](=[*:3]).[102*][*:4]'),
    	('Acylsulphamide', '[#6:1][S!R:2](=[O:3])(=[O:4])!@[N:5]!@[C!R:6](=[O:7])>>[*:1][*:2](=[*:3])(=[*:4])[*:5][103*].[104*][*:6](=[*:7])'),
    	('Sulfacylation-O', '[S!R:1](=[O:2])(=[O:3])!@[O:4]!@[c:5]>>[*:1](=[*:2])(=[*:3])[105*].[106*][*:4][*:5]'),
    	('CuAAC_triazole_break_1', '[c:1]-[n:2]>>[107*][*:1].[108*][*:2]'),
    	('CuAAC_triazole_break_2', '[n:1]@[n:2]>>[109*][*:1].[110*][*:2]'),
    	('SPAAC_break', '[c:1]-[C;R0:2]>>[111*][*:1].[112*][*:2]'),
    	('C-C break (aliphatic)', '[C;!R;!$(C=[O,N]):1]!@[C;!R;!$(C=[O,N]):2]>>[113*][*:1].[114*][*:2]'),
    	('C-hetero (aliphatic)', '[C;!R:1]!@[!#6;!R:2]>>[115*][*:1].[116*][*:2]'),
    	('Sulfonate ester cleavage', '[S:1](=O)(=O)-O-[C:2]>>[117*][*:1](=O)(=O)O.[118*][*:2]'),
    	('Disulfide bond cleavage', '[S:1]-[S:2]>>[119*][*:1].[120*][*:2]'),
    	('Ar-vinyl-strict', '[c:1]-[C:2]=[C:3]>>[121*][c:1].[122*][C:2]=[C:3]'),
    	('Ar-O (exocyclic-refined)', '[c:1]-[O:2]>>[123*][c:1].[124*][O:2]'),
    	('Ar-N (exocyclic-refined)', '[c:1]-[N:2]>>[125*][c:1].[126*][N:2]'),
    	('Ar-S (exocyclic-refined)', '[c:1]-[S:2]>>[127*][c:1].[128*][S:2]'),
    	('Ar-C3C (terminal)', '[c:1]-[C;D1:2]#[C:3]>>[129*][c:1].[130*][C:2]#[C:3]'),
    	('Ar-C3C (substituted)', '[c:1]-[C:2]#[C;!H:3]>>[131*][c:1].[132*][C:2]#[C:3]')
   ]



    if mode == "RECAP":
        rxn_smirks = recap_smirks
    elif mode == "BRICS":
        rxn_smirks = brics_smirks
    elif mode == "CRUSH":
        rxn_smirks = CRUSH_smirks
    else:
        raise ValueError("Mode must be one of: RECAP, BRICS, CRUSH")

    if random_seed is not None:
        random.seed(random_seed)
        random.shuffle(rxn_smirks)

    return rxn_smirks


# --------------------------
# Helper utilities
# --------------------------

def has_smaller_number(numbers, min_frag_size):
    return any(num < min_frag_size for num in numbers)

def tryCalcNumHeavyAtoms(mol):
    try:
        if mol is None:
            return 0
        return mol.GetNumHeavyAtoms()
    except Exception:
        return 0

def _atom_ring_count(mol):
    """Return dictionary {atom_idx: number_of_rings_including_the_atom}."""
    ring_info = mol.GetRingInfo().AtomRings()
    counts = {i: 0 for i in range(mol.GetNumAtoms())}
    for ring in ring_info:
        for idx in ring:
            counts[idx] += 1
    return counts


def read_fingerprints_in_chunks(filename, fp_bytes, batch_size):
    """
    Generator that yields batches of unpacked fingerprints as numpy arrays.
    """
    with open(filename, "rb") as f:
        while True:
            data = f.read(fp_bytes * batch_size)
            if not data:
                break
            n_fps = len(data) // fp_bytes
            arr = np.frombuffer(data, dtype=np.uint8).reshape(n_fps, fp_bytes)
            # unpack bits to 0/1
            fps = np.unpackbits(arr, axis=1)
            yield fps

# ------------------------------
# Core recursive fragmentation
# ------------------------------

def compile_reactions_for_mode(mode, random_seed=None):
    """
    Return list of tuples (name, ReactionObj) for the given mode,
    using the SMIRKS lists above.
    """
    rxn_smarts = frag_rules(mode=mode, random_seed=random_seed)
    reactions = []
    for name, smarts in rxn_smarts:
        try:
            rxn = AllChem.ReactionFromSmarts(smarts)
            reactions.append((name, rxn))
        except Exception:
            # skip invalid smirks silently (but could log)
            continue
    return reactions


def mol_fragmentation_single(smi, compiled_reactions, min_frag_size=3,
                             protect_neighbor_of_fused=False, conservative=True, debug=False):
    """
    Apply the *recursive* fragmentation algorithm to a single SMILES.
    Returns the best_path (list of (parent_smi, rxn_name, [product_smiles...])).
    This is adapted from the user's mol_fragmentation but uses the precompiled reactions
    for the selected mode (compiled_reactions should be a dict {mode: [(name, rxn), ...]}).
    """
    max_frag_count = 0
    best_path = []

    # Internal recursive function (same logic as original)
    def recursive_fragment(smis, path, reactions):
        nonlocal max_frag_count, best_path
        fragmented = False

        for smi_local in smis:
            mol = Chem.MolFromSmiles(smi_local)
            if mol is None:
                if debug:
                    print(f"[WARN] could not parse SMILES: {smi_local}")
                continue

            ring_counts = _atom_ring_count(mol)
            fused_atoms = {idx for idx, cnt in ring_counts.items() if cnt >= 2}
            neighbor_of_fused = set()
            if protect_neighbor_of_fused and fused_atoms:
                for idx in fused_atoms:
                    atom = mol.GetAtomWithIdx(idx)
                    for nb in atom.GetNeighbors():
                        neighbor_of_fused.add(nb.GetIdx())
            protected_atoms = fused_atoms.union(neighbor_of_fused)

            for rxn_name, rxn in reactions:
                try:
                    if rxn is None:
                        continue
                    if rxn.GetNumReactantTemplates() != 1:
                        continue
                    template = rxn.GetReactantTemplate(0)
                    matches = mol.GetSubstructMatches(template, useChirality=False)
                    if not matches:
                        continue

                    safe_matches = [m for m in matches if not any(idx in protected_atoms for idx in m)]
                    if conservative:
                        if any(any(idx in protected_atoms for idx in m) for m in matches):
                            continue
                    else:
                        if not safe_matches:
                            continue

                    rxn_products = rxn.RunReactants((mol,))
                    for products in rxn_products:
                        # convert product molecules to canonical SMILES
                        products_smiles = [Chem.MolToSmiles(prod, True) for prod in products]
                        ha_frag = [tryCalcNumHeavyAtoms(Chem.MolFromSmiles(prod)) for prod in products_smiles]
                        if has_smaller_number(ha_frag, min_frag_size):
                            continue

                        new_path = path + [(smi_local, rxn_name, products_smiles)]
                        total_frags = sum(len(frag_list) for _, _, frag_list in new_path)

                        if total_frags < max_frag_count:
                            continue

                        # Recurse on products
                        deeper_fragmented = recursive_fragment(products_smiles, new_path, reactions)
                        fragmented = True

                        if (not deeper_fragmented) and (total_frags > max_frag_count):
                            max_frag_count = total_frags
                            best_path = new_path.copy()

                except Exception:
                    # ignore failed reaction runs (as in original)
                    continue

        return fragmented

    # Try each reaction set (we expect compiled_reactions to be a list of (name,rxn))
    recursive_fragment([smi], [], compiled_reactions)
    return best_path

# --------------------------
# Fingerprint computation (packed bytes)
# --------------------------

from rdkit.Chem import MACCSkeys

def compute_packed_maccs_bytes(smiles):
    """
    Compute MACCS keys fingerprint (166 bits) and pack to bytes.
    Returns bytes or None on failure.
    """
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None

    fp = MACCSkeys.GenMACCSKeys(mol)  # ExplicitBitVect, 166 bits
    arr = np.zeros((FP_NBITS,), dtype=np.uint8)
    DataStructs.ConvertToNumpyArray(fp, arr)

    packed = np.packbits(arr)
    return packed.tobytes()


# --------------------------
# worker function (multiprocessing)
# --------------------------

def worker_fragment_task(args):
    """
    Worker receives: (identifier, smiles, modes_to_reactions, min_frag_size, protect_neighbor_of_fused, conservative)
    Returns: (metadata_rows_list, list_of_fp_bytes) where both lists are in the same sequence.
    """
    identifier, smi, modes_to_reactions, min_frag_size, protect_neighbor_of_fused, conservative = args
    metadata_rows = []
    fps_bytes = []

    try:
        # For each mode (RECAP, BRICS, CRUSH) apply recursive fragmentation and collect fragments
        for mode in modes_to_reactions:
            reactions = modes_to_reactions[mode]
            # run recursive fragmentation (returns best path list)
            best_path = mol_fragmentation_single(smi, reactions, min_frag_size=min_frag_size,
                                                 protect_neighbor_of_fused=protect_neighbor_of_fused,
                                                 conservative=conservative, debug=False)
            # best_path is list of (parent_smi, rxn_name, [product_smiles...])
            for parent_smi, rxn_name, frag_list in best_path:
                for frag in frag_list:
                    # compute fingerprint bytes for fragment
                    fp_bytes = compute_packed_maccs_bytes(frag)
                    if fp_bytes is None:
                        continue
                    fps_bytes.append(fp_bytes)
                    metadata_rows.append({
                        "Identifier": identifier,
                        "Original_SMILES": smi,
                        "Fragment_SMILES": frag,
                        "Rule": rxn_name,
                        "Mode": mode
                    })

    except Exception as e:
        # don't crash worker; return what we have and optionally log
        print(f"[worker error] id={identifier}, smi={smi}, err={e}")
        traceback.print_exc()

    return metadata_rows, fps_bytes


# --------------------------
# orchestrator (main process writer + task distributor)
# --------------------------

def fragment_and_stream_parallel(csv_path, sep, smiles_col, id_col,
                                 out_csv=OUT_CSV, fps_bin=OUT_BIN,
                                 batch_write=BATCH_WRITE, chunksize=CHUNKSIZE,
                                 n_workers=N_WORKERS, min_frag_size=3,
                                 protect_neighbor_of_fused=True, conservative=True,
                                 random_seed=42):
    """
    Main function:
      - Precompile reaction sets for RECAP, BRICS, CRUSH using the SMIRKS above.
      - Read input CSV in chunks.
      - Submit worker tasks via multiprocessing.Pool.
      - Write fingerprint bytes to fps_bin and metadata rows to out_csv in batches.
    """

    # compile reactions once
    modes_to_reactions = {}
    for mode in MODES:
        modes_to_reactions[mode] = compile_reactions_for_mode(mode, random_seed=random_seed)

    # prepare output files (remove if exist)
    if os.path.exists(fps_bin):
        os.remove(fps_bin)
    first_write = not os.path.exists(out_csv)
    if os.path.exists(out_csv):
        os.remove(out_csv)

    total_written = 0
    rows_buffer = []

    pool = mp.Pool(processes=n_workers)

    start = time.time()
    with open(fps_bin, "ab") as fbin:
        # streaming read
        reader = pd.read_csv(csv_path, sep=sep, usecols=[id_col, smiles_col], chunksize=chunksize)
        for chunk_idx, chunk in enumerate(reader):
            # create tasks for pool
            tasks = []
            for _, row in chunk.iterrows():
                tasks.append((row[id_col], row[smiles_col],
                              modes_to_reactions, min_frag_size,
                              protect_neighbor_of_fused, conservative))

            # distribute tasks (unordered for speed)
            for meta_rows, fps_list in pool.imap_unordered(worker_fragment_task, tasks):
                # write fingerprint bytes sequentially
                for b in fps_list:
                    fbin.write(b)

                # collect metadata
                if meta_rows:
                    rows_buffer.extend(meta_rows)
                    total_written += len(meta_rows)

                # flush metadata if buffer large enough
                if len(rows_buffer) >= batch_write:
                    df_chunk = pd.DataFrame(rows_buffer)
                    df_chunk.to_csv(out_csv, mode="a", index=False, header=first_write)
                    first_write = False
                    rows_buffer = []

                # lightweight progress log
                if total_written and total_written % (batch_write * 10) < len(meta_rows):
                    elapsed = (time.time() - start) / 60.0
                    print(f"[{time.strftime('%H:%M:%S')}] written fragments: {total_written:,}  elapsed: {elapsed:.2f} min")

        # final flush of metadata
        if rows_buffer:
            df_chunk = pd.DataFrame(rows_buffer)
            df_chunk.to_csv(out_csv, mode="a", index=False, header=first_write)
            rows_buffer = []

    pool.close()
    pool.join()

    elapsed_total = (time.time() - start) / 60.0
    print(f"\nFragmentation finished. Metadata saved to '{out_csv}'.")
    print(f"Binary fingerprints saved to '{fps_bin}'. Approximate fragments written: {total_written}")
    print(f"Elapsed time: {elapsed_total:.2f} minutes")

# --------------------------
# CLI / main
# --------------------------

if __name__ == "__main__":
    print("=== Parallel streaming fragmentation (using user-provided SMIRKS rules) ===")
    csv_path = input("Enter input CSV path: ").strip()
    sep = input("Enter CSV separator (default ','): ").strip() or ","
    smiles_col = input("Enter SMILES column name: ").strip()
    id_col = input("Enter identifier column name: ").strip()

    fragment_and_stream_parallel(csv_path, sep, smiles_col, id_col,
                                 out_csv=OUT_CSV, fps_bin=OUT_BIN,
                                 batch_write=BATCH_WRITE, chunksize=CHUNKSIZE,
                                 n_workers=N_WORKERS, min_frag_size=3,
                                 protect_neighbor_of_fused=True, conservative=True,
                                 random_seed=42)


# --------------------------
# Robust Incremental PCA + UMAP
# --------------------------

# ===========================================
# Robust Incremental PCA + UMAP implementation
# ===========================================

def run_incremental_pca_and_umap_robust(
    fps_bin_path=FPS_BIN,
    fp_bytes=FP_NBYTES,
    batch_size=BATCH_SIZE,
    n_components_pca=N_COMPONENTS_PCA,
    n_components_umap=N_COMPONENTS_UMAP,
    random_seed=RANDOM_SEED,
    out_pca=OUT_PCA,
    out_umap=OUT_UMAP,
    meta_csv=META_CSV,
    out_csv_embed=OUT_CSV_EMBED,
    umap_n_neighbors=15,
    umap_min_dist=0.1
):
    """
    Performs Incremental PCA followed by UMAP on binary fingerprints (.bin).
    Designed to handle large datasets efficiently with low RAM usage.
    """

    print("\n=== Running Incremental PCA + UMAP (Robust Version) ===\n")

    # ----------------------------------------------------------
    # Step 1 — Determine dataset size
    # ----------------------------------------------------------
    total_bytes = os.path.getsize(fps_bin_path)
    total_fps = total_bytes // fp_bytes
    print(f"[INFO] Total fingerprints detected: {total_fps:,} ({total_bytes / 1e6:.2f} MB total)")

    if total_fps == 0:
        raise RuntimeError("No fingerprints found in binary file. Aborting.")

    # Auto-adjust PCA components and batch size if needed
    if n_components_pca >= total_fps:
        n_components_pca = max(1, total_fps - 1)
        print(f"[WARN] n_components_pca adjusted to {n_components_pca} (must be < total samples).")

    if batch_size > total_fps:
        batch_size = total_fps
        print(f"[WARN] batch_size adjusted to {batch_size} (must be <= total samples).")

    # ----------------------------------------------------------
    # Step 2 — Fit Incremental PCA model
    # ----------------------------------------------------------
    print(f"[INFO] Fitting Incremental PCA (n_components={n_components_pca}, batch_size={batch_size})...")
    ipca = IncrementalPCA(n_components=n_components_pca, batch_size=batch_size)
    processed = 0

    for fps in read_fingerprints_in_chunks(fps_bin_path, fp_bytes, batch_size):
        ipca.partial_fit(fps)
        processed += fps.shape[0]
        if processed % (batch_size * 5) == 0 or processed == total_fps:
            print(f"[INFO] IncrementalPCA fit progress: {processed:,}/{total_fps:,} fingerprints")

    print("[INFO] Incremental PCA fitting complete.")

    # ----------------------------------------------------------
    # Step 3 — Transform fingerprints to PCA embeddings
    # ----------------------------------------------------------
    print("[INFO] Transforming fingerprints into PCA space (batchwise)...")
    pca_chunks = []
    processed = 0

    for fps in read_fingerprints_in_chunks(fps_bin_path, fp_bytes, batch_size):
        reduced = ipca.transform(fps)
        pca_chunks.append(reduced)
        processed += fps.shape[0]
        if processed % (batch_size * 5) == 0 or processed == total_fps:
            print(f"[INFO] PCA transform progress: {processed:,}/{total_fps:,}")

    X_pca = np.vstack(pca_chunks)
    np.save(out_pca, X_pca)
    print(f"[INFO] PCA embeddings saved → {out_pca} | shape={X_pca.shape}")

    # ----------------------------------------------------------
    # Step 4 — Run UMAP on PCA embeddings
    # ----------------------------------------------------------
    print("[INFO] Running UMAP on PCA embeddings...")
    reducer = umap.UMAP(
        n_components=n_components_umap,
        n_neighbors=umap_n_neighbors,
        min_dist=umap_min_dist,
        metric="euclidean",
        init="random",       # avoids heavy spectral initialization
        random_state=random_seed,
        verbose=True
    )

    try:
        X_umap = reducer.fit_transform(X_pca)
    except MemoryError as e:
        print("[ERROR] UMAP ran out of memory. Try reducing n_components_pca or sample size.")
        raise e

    np.save(out_umap, X_umap)
    print(f"[INFO] UMAP embeddings saved → {out_umap} | shape={X_umap.shape}")

    # ----------------------------------------------------------
    # Step 5 — Merge UMAP coordinates with fragment metadata
    # ----------------------------------------------------------
    print("[INFO] Merging metadata with embeddings...")
    meta_df = pd.read_csv(meta_csv)

    if len(meta_df) != X_umap.shape[0]:
        print(f"[WARN] Metadata rows ({len(meta_df)}) and embeddings ({X_umap.shape[0]}) differ.")
        min_len = min(len(meta_df), X_umap.shape[0])
        meta_df = meta_df.iloc[:min_len]
        X_umap = X_umap[:min_len]

    meta_df["UMAP_1"] = X_umap[:, 0]
    meta_df["UMAP_2"] = X_umap[:, 1]
    meta_df.to_csv(out_csv_embed, index=False)
    print(f"[INFO] Combined metadata with UMAP saved → {out_csv_embed}")

    print("\n✅ [DONE] Incremental PCA + UMAP completed successfully.\n")
    return X_pca, X_umap


# ===========================================
# Optional: main entry point
# ===========================================

if __name__ == "__main__":
    print("\n=== Parallel Fragmentation + Incremental PCA + UMAP ===")
    choice = input("Run PCA+UMAP after fragmentation? (y/n): ").strip().lower()
    if choice == "y":
        run_incremental_pca_and_umap_robust(
            fps_bin_path=OUT_BIN,
            fp_bytes=FP_NBYTES,
            batch_size=BATCH_SIZE,
            n_components_pca=N_COMPONENTS_PCA,
            n_components_umap=N_COMPONENTS_UMAP,
            random_seed=RANDOM_SEED,
            out_pca=OUT_PCA,
            out_umap=OUT_UMAP,
            meta_csv=OUT_CSV,
            out_csv_embed=OUT_CSV_EMBED
        )
    else:
        print("Skipping PCA+UMAP step. Fragmentation output is ready for later use.")

